{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import transform\n",
    "from dataset_loader import DatasetLoader\n",
    "from annotations_loader import AnnotationsLoader\n",
    "\n",
    "def display_img(img, BGR=False):\n",
    "    if isinstance(img, str):\n",
    "        display_img(cv2.imread(img))\n",
    "        return\n",
    "    plt.figure(dpi=150)\n",
    "    if BGR:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/proj/brizk/output/'\n",
    "faces_dir = os.path.join(*[home_dir, 'retinaface'])\n",
    "attention_dir = os.path.join(*[home_dir, 'attentiontarget'])\n",
    "openpose_dir = os.path.join(*[home_dir, 'openpose'])\n",
    "videos_loader = DatasetLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = next(videos_loader)\n",
    "video.filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num = (2471)\n",
    "with open(os.path.join(*[openpose_dir, video.name, f'{video.name}_{\"%012d\"%frame_num}_keypoints.json']), mode='rb') as f:\n",
    "    openpose_annotation = json.load(f)\n",
    "openpose_annotation['people'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(openpose_annotation['people'][0]['pose_keypoints_2d'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_SEG_THRESH = 5\n",
    "CONF_THRESH = 0.2\n",
    "LEAST_SEG_LEN = 25\n",
    "NAME = 'rect_RGB_5_6_0000000'\n",
    "\n",
    "\n",
    "def json_to_poses(json_file):\n",
    "    data = json.load(open(json_file,'rb'))\n",
    "    people = data['people']\n",
    "    poses = []\n",
    "    num_ppl = len(people)\n",
    "    for i in range(num_ppl):\n",
    "        pose_ = np.array(people[i]['pose_keypoints_2d'])\n",
    "        pose_ = np.reshape(pose_, (25,3))\n",
    "        poses.append(pose_)\n",
    "    return poses\n",
    "    \n",
    "def num_people_stats(video, segment=None, mask=None):\n",
    "    json_dir = os.path.join(openpose_dir, video.name)\n",
    "    if segment is None:\n",
    "        # num_frames = video.org_frame_count\n",
    "        num_frames = len(os.listdir(json_dir))\n",
    "        ##add the range for the frames present in the folder\n",
    "        num_poses = []\n",
    "        for i in tqdm(range(num_frames), desc='Reading Frames'):\n",
    "            frame_file_name = os.path.join(json_dir, f'{video.name}_{frame_str(i)}_keypoints.json')\n",
    "            poses = json_to_poses(frame_file_name)\n",
    "            num_poses.extend([len(poses)])\n",
    "        return np.array(num_poses)\n",
    "    elif mask is None:\n",
    "        num_frames = segment[1] - segment[0]\n",
    "        # seg_nums = np.arange(segment[0], segment[1])\n",
    "        num_poses = []\n",
    "        for i in range(num_frames):\n",
    "            frame_file_name = os.path.join(json_dir, f'{video.name}_{frame_str(i)}_keypoints.json')\n",
    "            poses = json_to_poses(frame_file_name)\n",
    "            num_poses.extend([len(poses)])\n",
    "        return np.array(num_poses)\n",
    "    else:\n",
    "        num_frames = segment[1] - segment[0]\n",
    "        # seg_nums = np.arange(segment[0], segment[1])\n",
    "        num_poses = []\n",
    "        for i in range(num_frames):\n",
    "            if mask[segment[i]]:\n",
    "                frame_file_name = os.path.join(json_dir, f'{video.name}_{frame_str(i)}_keypoints.json')\n",
    "                poses = json_to_poses(frame_file_name)\n",
    "                num_poses.extend([len(poses)])\n",
    "        return np.array(num_poses)\n",
    "\n",
    "def create_segs(video):\n",
    "    num_poses_per_frame = num_people_stats(video)\n",
    "    iszero = np.concatenate(([0], np.equal(num_poses_per_frame, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    len_zeros = ranges[:,1] - ranges[:,0]\n",
    "    segments = []\n",
    "    seg_ = [0,0]\n",
    "    for i in range(len_zeros.shape[0]):\n",
    "        if len_zeros[i] > POSE_SEG_THRESH:\n",
    "            seg_[1] = ranges[i,0]\n",
    "            segments.append(seg_)\n",
    "            seg_ = [ranges[i,1], ranges[i,1]]\n",
    "    seg_[1] = num_poses_per_frame.shape[0]\n",
    "    segments.append(seg_)\n",
    "    segments = np.array(segments)\n",
    "    seg_len = segments[:,1] - segments[:,0]\n",
    "    segments = segments[seg_len > LEAST_SEG_LEN]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = create_segs(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_img = cv2.cvtColor(video[frame_num], cv2.COLOR_BGR2RGB)\n",
    "print('queried frame_num', frame_num)\n",
    "print('current frame', video.current_frame_num)\n",
    "height, width, _ = frame_img.shape\n",
    "# faces_per_frame = annotation['faces']\n",
    "# attention_per_frame = annotation['attention']\n",
    "# annotation['faces']\n",
    "display_img(frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    video = next(videos_loader)\n",
    "    video_name = os.path.basename(video.filepath).split('.')[0]\n",
    "    ds = videos_loader.current_ds\n",
    "    print(f'Processing {video_name} @ {ds}')\n",
    "    try:\n",
    "        annotations = AnnotationsLoader(\n",
    "            ds, video_name, faces_dir=faces_dir, attention_dir=attention_dir,\n",
    "            faces_confidence_thres = 0.95\n",
    "        )\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        print(f'Annotations not available yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over frames annotations (Targeting only at least of 2 attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = AnnotationsLoader(\n",
    "            ds, video_name, faces_dir=faces_dir, attention_dir=attention_dir,\n",
    "            faces_confidence_thres = 0.95\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed = 0\n",
    "faces_num_thres = 2\n",
    "num_of_faces = 0\n",
    "\n",
    "while(True):\n",
    "    annotation = next(annotations)\n",
    "    if annotation is None:\n",
    "        print('Reached End')\n",
    "        break\n",
    "    num_of_faces = len(annotation['faces'])\n",
    "    if num_of_faces >= faces_num_thres:\n",
    "        break\n",
    "    passed += 1\n",
    "\n",
    "if num_of_faces >= faces_num_thres:\n",
    "    print('Discarded', passed, 'frames')\n",
    "    frame_num = annotation['frame']\n",
    "    print('Now at frame', frame_num)\n",
    "else:\n",
    "    print(f'Did not found any simulatanious at least {faces_num_thres} faces in video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation['faces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_face_center(head_box):\n",
    "    return [\n",
    "        head_box[0] + (head_box[2] - head_box[0])//2,\n",
    "        head_box[1] + (head_box[3] - head_box[1])//2\n",
    "    ]\n",
    "\n",
    "def get_attention_vector(head_box, observation_coordinates):\n",
    "    face_center = calc_face_center(head_box)\n",
    "    return np.append(face_center, observation_coordinates)\n",
    "\n",
    "def calc_line_angle(line):\n",
    "    return np.degrees(np.arctan2(-(line[3]-line[1]), line[2]-line[0]))\n",
    "\n",
    "def calc_relative_angle(vectors, verbose=False):\n",
    "    angles = np.array([calc_line_angle(l) for l in vectors])\n",
    "    if verbose:\n",
    "        print('Angles:', angles)\n",
    "    return abs(angles[0] - angles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_img = cv2.cvtColor(video[frame_num], cv2.COLOR_BGR2RGB)\n",
    "print('queried frame_num', frame_num)\n",
    "print('current frame', video.current_frame_num)\n",
    "height, width, _ = frame_img.shape\n",
    "faces_per_frame = annotation['faces']\n",
    "attention_per_frame = annotation['attention']\n",
    "annotation['faces']\n",
    "\n",
    "display_img(frame_img)\n",
    "\n",
    "colors = [(255, 0, 0), (0, 0, 255), (0, 255, 0)]\n",
    "attention_vectors = np.zeros((num_of_faces,4), dtype=np.int32)\n",
    "\n",
    "for i in faces_per_frame.index:\n",
    "    head_box = np.array(\n",
    "        [faces_per_frame.loc[i,'left'], faces_per_frame.loc[i,'top'],\n",
    "         faces_per_frame.loc[i,'right'], faces_per_frame.loc[i,'bottom']]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    observation_coordinates = attention_per_frame.loc[i, 'observation_coordinates']\n",
    "    print('Observation is at', observation_coordinates)\n",
    "\n",
    "    raw_hm = attention_per_frame.loc[i, 'raw_hm']\n",
    "    inout = attention_per_frame.loc[i, 'inout']\n",
    "    norm_map = transform.resize(raw_hm, (height, width)) - inout\n",
    "\n",
    "\n",
    "    vector_of_attention = get_attention_vector(head_box, observation_coordinates)\n",
    "    print('vector of attention:', vector_of_attention)\n",
    "    cv2.line(frame_img, vector_of_attention[:2], vector_of_attention[2:], colors[i], 4)\n",
    "    attention_vectors[i] = vector_of_attention\n",
    "    \n",
    "    cv2.rectangle(frame_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), colors[i], 4)\n",
    "    cv2.circle(frame_img, observation_coordinates, int(height/50.0), colors[i], 4)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    circ = patches.Circle((observation_coordinates[0], observation_coordinates[1]), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "    ax.add_patch(circ)\n",
    "    \n",
    "    plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)\n",
    "    # Grayscale then Otsu's threshold\n",
    "    gray = norm_map.copy()\n",
    "    gray[gray < 0] = 0\n",
    "    gray = gray.astype(np.uint8)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        # print('contour shape', c.shape)\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame_img, (x, y), (x + w, y + h), colors[i], 10)\n",
    "        print('rect bounds', x, y, w, h)\n",
    "        \n",
    "    plt.plot((observation_coordinates[0],(head_box[0]+head_box[2])/2), (observation_coordinates[1],(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "display_img(frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing two vectors looking opposite to each other\n",
    "attention_vectors_dump = attention_vectors.copy()\n",
    "attention_vectors_dump[1] = attention_vectors_dump[0].copy()\n",
    "tmp = attention_vectors_dump[0][:2].copy()\n",
    "attention_vectors_dump[0][:2] = attention_vectors_dump[0][2:].copy()\n",
    "attention_vectors_dump[0][2:] = tmp\n",
    "print(attention_vectors_dump)\n",
    "calc_relative_angle(attention_vectors_dump, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention_vectors)\n",
    "calc_relative_angle(attention_vectors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('retina_face')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faec6b7559de2b045bda80a9afcf29ca164f2f5fedec1d0a28853338c5a0785b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
