{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from dataset_loader import DatasetLoader\n",
    "from annotations_loader import AnnotationsLoader\n",
    "\n",
    "def display_img(img, BGR=False):\n",
    "    if isinstance(img, str):\n",
    "        display_img(cv2.imread(img))\n",
    "        return\n",
    "    plt.figure(dpi=150)\n",
    "    if BGR:\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        plt.imshow(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_loader = DatasetLoader()\n",
    "# video = videos_ds[(ds, video_name)]\n",
    "video = videos_loader[('MS', '2016-09-16_IDMSSM28_BOSCC_vid')]\n",
    "video_name = os.path.basename(video.filepath).split('.')[0]\n",
    "ds = videos_loader.current_ds\n",
    "\n",
    "home_dir = '/proj/brizk/output/'\n",
    "faces_dir = os.path.join(*[home_dir, 'retinaface'])\n",
    "attention_dir = os.path.join(*[home_dir, 'attentiontarget'])\n",
    "\n",
    "annotations = AnnotationsLoader(\n",
    "    ds, video_name, faces_dir=faces_dir, attention_dir=attention_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriving one image frame and its annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = next(annotations)\n",
    "df = annotations.faces_file\n",
    "df['left'] -= (df['right']-df['left'])*0.1\n",
    "df['right'] += (df['right']-df['left'])*0.1\n",
    "df['top'] -= (df['bottom']-df['top'])*0.1\n",
    "df['bottom'] += (df['bottom']-df['top'])*0.1\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('current frame before query', video.current_frame_num)\n",
    "frame_num = annotation['frame']\n",
    "frame_img = cv2.cvtColor(video[frame_num], cv2.COLOR_BGR2RGB)\n",
    "print('queried frame_num', frame_num)\n",
    "print('current frame', video.current_frame_num)\n",
    "height, width, _ = frame_img.shape\n",
    "\n",
    "faces_per_frame = annotation['faces'].reset_index()\n",
    "attention_per_frame = annotation['attention']\n",
    "head_boxes = []\n",
    "for i in faces_per_frame.index:\n",
    "    head_boxes.append(np.array(\n",
    "        [faces_per_frame.loc[i,'left'], faces_per_frame.loc[i,'top'],\n",
    "         faces_per_frame.loc[i,'right'], faces_per_frame.loc[i,'bottom']]\n",
    "    ).astype(np.int32))\n",
    "\n",
    "head_box = head_boxes[0]\n",
    "print(head_boxes)\n",
    "display_img(frame_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing attention target over one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_dir = f'/proj/brizk/output/attentiontarget/{ds}/{video_name}.pkl'\n",
    "# with open(src_dir, 'rb') as f:    \n",
    "#     attention_target_obj = pickle.load(f)   \n",
    "    \n",
    "i = 0\n",
    "observation_coordinates = attention_per_frame[i]['observation_coordinates']\n",
    "raw_hm = attention_per_frame[i]['raw_hm']\n",
    "inout = attention_per_frame[i]['inout']\n",
    "norm_map = transform.resize(raw_hm, (height, width)) - inout\n",
    "\n",
    "\n",
    "cv2.rectangle(frame_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), (0, 255, 0), 4)\n",
    "cv2.circle(frame_img, observation_coordinates, int(height/50.0), (0, 255, 0), 4)\n",
    "print('Observation is at', observation_coordinates)\n",
    "\n",
    "display_img(frame_img)\n",
    "ax = plt.gca()\n",
    "rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "circ = patches.Circle((observation_coordinates[0], observation_coordinates[1]), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "ax.add_patch(circ)\n",
    "plt.plot((observation_coordinates[0],(head_box[0]+head_box[2])/2), (observation_coordinates[1],(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "\n",
    "plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Bound Box from Heatmap using Numpy and CV2 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_copy = frame_img.copy()\n",
    "\n",
    "min = np.min(norm_map)\n",
    "max = np.max(norm_map)\n",
    "print(f'norm_map min {min} max {max}')\n",
    "# Grayscale then Otsu's threshold\n",
    "gray = norm_map.copy()\n",
    "gray[gray < 0] = 0\n",
    "print(gray.shape)\n",
    "gray = gray.astype(np.uint8)\n",
    "print(gray.shape)\n",
    "print(f'unint8 conversion - min {np.min(gray)} max {np.max(gray)}')\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "print(f'thresh - min {np.min(thresh)} max {np.max(thresh)}')\n",
    "print(thresh.shape)\n",
    "\n",
    "\n",
    "# Find contours\n",
    "cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "for c in cnts:\n",
    "    print('contour shape', c.shape)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    cv2.rectangle(thresh, (x, y), (x + w, y + h), (155, 0,0), 10)\n",
    "    cv2.rectangle(img_copy, (x, y), (x + w, y + h), (155,0,0), 10)\n",
    "    print('rect bounds', x, y, w, h)\n",
    "\n",
    "display_img(thresh, BGR=True)\n",
    "display_img(img_copy)\n",
    "display_img(norm_map, BGR=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Bound Box from Heatmap using /proj/rash/CAM-Python/BBoxGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is the code to generate the bounding box from the heatmap\n",
    "# \n",
    "# to reproduce the ILSVRC localization result, you need to first generate\n",
    "# the heatmap for each testing image by merging the heatmap from the\n",
    "# 10-crops (it is exactly what the demo code is doing), then resize the merged heatmap back to the original size of\n",
    "# that image. Then use this bbox generator to generate the bbox from the resized heatmap.\n",
    "#\n",
    "# The source code of the bbox generator is also released. Probably you need\n",
    "# to install the correct version of OpenCV to compile it.\n",
    "#\n",
    "# Special thanks to Hui Li for helping on this code.\n",
    "#\n",
    "# Bolei Zhou, April 19, 2016\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "REPO_PATH = '/proj/rash/CAM-Python'\n",
    "sys.path.append(REPO_PATH)\n",
    "\n",
    "from py_map2jpg import py_map2jpg\n",
    "\n",
    "def im2double(im):\n",
    "\treturn cv2.normalize(im.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "bbox_threshold = [20, 100, 110] # parameters for the bbox generator\n",
    "curParaThreshold = str(bbox_threshold[0])+' '+str(bbox_threshold[1])+' '+str(bbox_threshold[2])+' '\n",
    "curHeatMapFile = 'heatmap_6.jpg'\n",
    "curBBoxFile = 'heatmap_6.txt'\n",
    "\n",
    "cv2.imwrite(curHeatMapFile, norm_map)\n",
    "\n",
    "os.system(REPO_PATH + \"/bboxgenerator/./dt_box \"+curHeatMapFile+' '+curParaThreshold+' '+curBBoxFile)\n",
    "\n",
    "with open(curBBoxFile) as f:\n",
    "\tfor line in f:\n",
    "\t\titems = [int(x) for x in line.strip().split()]\n",
    "\n",
    "boxData1 = np.array(items[0::4]).T\n",
    "boxData2 = np.array(items[1::4]).T\n",
    "boxData3 = np.array(items[2::4]).T\n",
    "boxData4 = np.array(items[3::4]).T\n",
    "\n",
    "boxData_formulate = np.array([boxData1, boxData2, boxData1+boxData3, boxData2+boxData4]).T\n",
    "\n",
    "col1 = np.min(np.array([boxData_formulate[:,0], boxData_formulate[:,2]]), axis=0)\n",
    "col2 = np.min(np.array([boxData_formulate[:,1], boxData_formulate[:,3]]), axis=0)\n",
    "col3 = np.max(np.array([boxData_formulate[:,0], boxData_formulate[:,2]]), axis=0)\n",
    "col4 = np.max(np.array([boxData_formulate[:,1], boxData_formulate[:,3]]), axis=0)\n",
    "\n",
    "boxData_formulate = np.array([col1, col2, col3, col4]).T\n",
    "\n",
    "curHeatMap = cv2.imread(curHeatMapFile)\n",
    "curImg = frame_img\n",
    "\n",
    "curHeatMap = im2double(curHeatMap)\n",
    "curHeatMap = py_map2jpg(curHeatMap, None, 'jet')\n",
    "curHeatMap = im2double(curImg)*0.2+im2double(curHeatMap)*0.7\n",
    "\n",
    "for i in range(boxData_formulate.shape[0]): # for each bbox\n",
    "\tprint(boxData_formulate[i][:2])\n",
    "\tprint(boxData_formulate[i][2:])\n",
    "\tcv2.rectangle(curHeatMap, tuple(boxData_formulate[i][:2]), tuple(boxData_formulate[i][2:]), (255,0,0), 3)\n",
    "\tdisplay_img(curHeatMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing all attentions of one frame in single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = annotations[5000]\n",
    "\n",
    "print('current frame before query', video.current_frame_num)\n",
    "frame_num = annotation['frame']\n",
    "frame_img = cv2.cvtColor(video[frame_num], cv2.COLOR_BGR2RGB)\n",
    "print('queried frame_num', frame_num)\n",
    "print('current frame', video.current_frame_num)\n",
    "height, width, _ = frame_img.shape\n",
    "\n",
    "faces_per_frame = annotation['faces'].reset_index()\n",
    "attention_per_frame = annotation['attention']\n",
    "\n",
    "\n",
    "display_img(frame_img)\n",
    "for i in faces_per_frame.index:\n",
    "    head_box = np.array(\n",
    "        [faces_per_frame.loc[i,'left'], faces_per_frame.loc[i,'top'],\n",
    "         faces_per_frame.loc[i,'right'], faces_per_frame.loc[i,'bottom']]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    observation_coordinates = attention_per_frame[i]['observation_coordinates']\n",
    "    raw_hm = attention_per_frame[i]['raw_hm']\n",
    "    inout = attention_per_frame[i]['inout']\n",
    "    norm_map = transform.resize(raw_hm, (height, width)) - inout\n",
    "\n",
    "    face_center = [head_box[2] - head_box[0], head_box[3] - head_box[1]]\n",
    "    \n",
    "    \n",
    "     \n",
    "    cv2.rectangle(frame_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), (0, 255, 0), 4)\n",
    "    cv2.circle(frame_img, observation_coordinates, int(height/50.0), (0, 255, 0), 4)\n",
    "    print('Observation is at', observation_coordinates)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    circ = patches.Circle((observation_coordinates[0], observation_coordinates[1]), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "    ax.add_patch(circ)\n",
    "    \n",
    "    plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)\n",
    "    # Grayscale then Otsu's threshold\n",
    "    gray = norm_map.copy()\n",
    "    gray[gray < 0] = 0\n",
    "    gray = gray.astype(np.uint8)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        # print('contour shape', c.shape)\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame_img, (x, y), (x + w, y + h), (255,0 ,0), 10)\n",
    "        print('rect bounds', x, y, w, h)\n",
    "        \n",
    "    plt.plot((observation_coordinates[0],(head_box[0]+head_box[2])/2), (observation_coordinates[1],(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "display_img(frame_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over frames annotations (Targeting only at least of 2 attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_img(frame_img)\n",
    "for i in faces_per_frame.index:\n",
    "    head_box = np.array(\n",
    "        [faces_per_frame.loc[i,'left'], faces_per_frame.loc[i,'top'],\n",
    "         faces_per_frame.loc[i,'right'], faces_per_frame.loc[i,'bottom']]\n",
    "    ).astype(np.int32)\n",
    "    \n",
    "    observation_coordinates = attention_per_frame[i]['observation_coordinates']\n",
    "    raw_hm = attention_per_frame[i]['raw_hm']\n",
    "    inout = attention_per_frame[i]['inout']\n",
    "    norm_map = transform.resize(raw_hm, (height, width)) - inout\n",
    "\n",
    "    face_center = [head_box[2] - head_box[0], head_box[3] - head_box[1]]\n",
    "    \n",
    "    \n",
    "     \n",
    "    cv2.rectangle(frame_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), (0, 255, 0), 4)\n",
    "    cv2.circle(frame_img, observation_coordinates, int(height/50.0), (0, 255, 0), 4)\n",
    "    print('Observation is at', observation_coordinates)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    circ = patches.Circle((observation_coordinates[0], observation_coordinates[1]), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "    ax.add_patch(circ)\n",
    "    \n",
    "    plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)\n",
    "    # Grayscale then Otsu's threshold\n",
    "    gray = norm_map.copy()\n",
    "    gray[gray < 0] = 0\n",
    "    gray = gray.astype(np.uint8)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    # Find contours\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        # print('contour shape', c.shape)\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame_img, (x, y), (x + w, y + h), (255,0 ,0), 10)\n",
    "        print('rect bounds', x, y, w, h)\n",
    "        \n",
    "    plt.plot((observation_coordinates[0],(head_box[0]+head_box[2])/2), (observation_coordinates[1],(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "display_img(frame_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('retina_face')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faec6b7559de2b045bda80a9afcf29ca164f2f5fedec1d0a28853338c5a0785b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
