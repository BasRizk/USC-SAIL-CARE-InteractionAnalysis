{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import sys\n",
    "REPO_PATH = '/proj/brizk/attention-target-detection'\n",
    "sys.path.append(REPO_PATH)\n",
    "from config import input_resolution, output_resolution\n",
    "from utils import imutils, evaluation\n",
    "from model import ModelSpatial\n",
    "import torch\n",
    "\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "from dataset_loader import DatasetLoader, RetinafaceInferenceGenerator\n",
    "\n",
    "def display_img(img):\n",
    "    if isinstance(img, str):\n",
    "        display_img(cv2.imread(img))\n",
    "        return\n",
    "    plt.figure(dpi=100)\n",
    "    plt.imshow(np.array(img))\n",
    "    # px.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/proj/brizk/output/'\n",
    "faces_dir = os.path.join(*[home_dir, 'retinaface'])\n",
    "faces_files = RetinafaceInferenceGenerator(faces_dir)\n",
    "ds, filename = next(faces_files)\n",
    "(faces_dir, ds, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = filename.split(\".\")[0]\n",
    "\n",
    "column_names = ['frame', 'confidence', 'left', 'top', 'right', 'bottom']\n",
    "   \n",
    "df = pd.read_csv(\n",
    "    os.path.join(*[faces_dir, ds, filename]),\n",
    "    header=None, names=column_names, usecols=range(6)\n",
    ")\n",
    "\n",
    "df['left'] -= (df['right']-df['left'])*0.1\n",
    "df['right'] += (df['right']-df['left'])*0.1\n",
    "df['top'] -= (df['bottom']-df['top'])*0.1\n",
    "df['bottom'] += (df['bottom']-df['top'])*0.1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_ds = DatasetLoader()\n",
    "video = videos_ds[(ds, video_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1500\n",
    "frame_num = df.loc[i, 'frame']\n",
    "# print(frame_num)\n",
    "# print(video.current_frame_num)\n",
    "# while(video.current_frame_num < frame_num):\n",
    "#     frame_img = next(video)\n",
    "frame_img = video[frame_num]\n",
    "print(video.current_frame_num)\n",
    "frame_raw = Image.fromarray(cv2.cvtColor(frame_img, cv2.COLOR_BGR2RGB))\n",
    "# frame_raw = Image.open(os.path.join(*[home_dir, 'imgs', 'MS' , video_name, str(frame_num) + \".jpg\" ]))\n",
    "width, height = frame_raw.size\n",
    "frame_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_transform():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Resize((input_resolution, input_resolution)))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data transformation\n",
    "test_transforms = _get_transform()\n",
    "\n",
    "head_box = np.array([df.loc[i,'left'], df.loc[i,'top'], df.loc[i,'right'], df.loc[i,'bottom']]).astype(np.int32)\n",
    "head_img = frame_raw.crop((head_box)) # head crop\n",
    "head = test_transforms(head_img) # transform inputs\n",
    "frame = test_transforms(frame_raw)\n",
    "head_channel = imutils.get_head_box_channel(head_box[0], head_box[1], head_box[2], head_box[3], width, height,\n",
    "                                            resolution=input_resolution).unsqueeze(0)\n",
    "head = head.unsqueeze(0).cuda()\n",
    "frame = frame.unsqueeze(0).cuda()\n",
    "head_channel = head_channel.unsqueeze(0).cuda()\n",
    "\n",
    "print(head.shape)\n",
    "head_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSpatial()\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = torch.load(os.path.join(REPO_PATH, 'model_demo.pt'))\n",
    "pretrained_dict = pretrained_dict['model']\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.cuda()\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # forward pass\n",
    "    raw_hm, _, inout = model(frame, head_channel, head)\n",
    "\n",
    "    # # heatmap modulation\n",
    "    raw_hm = raw_hm.cpu().detach().numpy() * 255\n",
    "    raw_hm = raw_hm.squeeze()\n",
    "    inout = inout.cpu().detach().numpy()\n",
    "    inout = 1 / (1 + np.exp(-inout))\n",
    "    inout = (1 - inout) * 255\n",
    "    norm_map = imresize(raw_hm, (height, width)) - inout\n",
    "    # pred_x, pred_y = evaluation.argmax_pts(raw_hm)\n",
    "    # norm_p = [pred_x/output_resolution, pred_y/output_resolution]\n",
    "    # observation_coordinates = tuple(map(int, (norm_p[0]*width, norm_p[1]*height)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.gca().set_axis_off()\n",
    "# plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "#             hspace = 0, wspace = 0)\n",
    "# plt.margins(0,0)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(frame_raw)\n",
    "ax = plt.gca()\n",
    "rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "# if args.vis_mode == 'arrow':\n",
    "# if inout < args.out_threshold: # in-frame gaze\n",
    "pred_x, pred_y = evaluation.argmax_pts(raw_hm)\n",
    "norm_p = [pred_x/output_resolution, pred_y/output_resolution]\n",
    "circ = patches.Circle((norm_p[0]*width, norm_p[1]*height), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "ax.add_patch(circ)\n",
    "plt.plot((norm_p[0]*width,(head_box[0]+head_box[2])/2), (norm_p[1]*height,(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "\n",
    "# plt.savefig(str(i) + \"_out.jpg\", dpi=1200)\n",
    "\n",
    "# else:\n",
    "\n",
    "plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)\n",
    "\n",
    "\n",
    "# plt.show(block=False)\n",
    "# plt.pause(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = np.array(frame_raw)\n",
    "\n",
    "cv2.rectangle(out_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), (0, 255, 0), 4)\n",
    "\n",
    "pred_x, pred_y = evaluation.argmax_pts(raw_hm)\n",
    "norm_p = [pred_x/output_resolution, pred_y/output_resolution]\n",
    "observation_coordinates = tuple(map(int, (norm_p[0]*width, norm_p[1]*height)))\n",
    "cv2.circle(out_img, observation_coordinates, int(height/50.0), (0, 255, 0), 4)\n",
    "\n",
    "print('Observation is at', observation_coordinates)\n",
    "\n",
    "display_img(out_img)\n",
    "# cv2.applyColorMap(out_img, norm_map, cv2.COLORMAP_JET)\n",
    "\n",
    "# norm_map[norm_map < 0] = 0\n",
    "# cv2.addWeighted(out_img, 0.2, norm_map.astype(np.int32) , 0.1, 0)\n",
    "\n",
    "# px.imshow(np.array(out_img)).show() \n",
    "# cv2.imwrite(str(frame_num) + \"_out.jpg\", cv2.cvtColor(out_img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_x = observation_coordinates[0]\n",
    "o_y = observation_coordinates[1]\n",
    "print((o_x, o_y))\n",
    "print(norm_map.shape)\n",
    "\n",
    "step= 1\n",
    "snippet = norm_map[o_x-step:o_x+step+1, o_y-step:o_y +step+1]\n",
    "print(snippet.shape)\n",
    "snippet\n",
    "\n",
    "# NOTE!!!! I do not understand how come is the observation coordinates map to negative value in the norm_map\n",
    "print(norm_map[o_y, o_x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 255/2.0\n",
    "mask = norm_map.copy()\n",
    "print(mask[o_y, o_x])\n",
    "\n",
    "mask[mask < thres] = 0\n",
    "mask = mask/255.0\n",
    "\n",
    "mask = mask.reshape((mask.shape[0], mask.shape[1], 1))\n",
    "\n",
    "# mask\n",
    "display_img(frame_raw*mask)\n",
    "\n",
    "\n",
    "# np.save('heatmap', raw_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "src_dir = '/proj/brizk/output/attentiontarget/MS/2016-09-16_IDMSSM28_BOSCC_vid.pkl'\n",
    "with open(src_dir, 'rb') as f:    \n",
    "    attention_target_obj = pickle.load(f)   \n",
    "    \n",
    "observation_coordinates = attention_target_obj[0]['observation_coordinates']\n",
    "raw_hm = attention_target_obj[0]['raw_hm']\n",
    "norm_map = imresize(raw_hm, (height, width)) - inout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = np.array(frame_raw)\n",
    "cv2.rectangle(out_img, (head_box[0], head_box[1]), (head_box[2], head_box[3]), (0, 255, 0), 4)\n",
    "cv2.circle(out_img, observation_coordinates, int(height/50.0), (0, 255, 0), 4)\n",
    "print('Observation is at', observation_coordinates)\n",
    "\n",
    "display_img(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(frame_raw)\n",
    "ax = plt.gca()\n",
    "rect = patches.Rectangle((head_box[0], head_box[1]), head_box[2]-head_box[0], head_box[3]-head_box[1], linewidth=2, edgecolor=(0,1,0), facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "circ = patches.Circle((observation_coordinates[0], observation_coordinates[1]), height/50.0, facecolor=(0,1,0), edgecolor='none')\n",
    "ax.add_patch(circ)\n",
    "plt.plot((observation_coordinates[0],(head_box[0]+head_box[2])/2), (observation_coordinates[1],(head_box[1]+head_box[3])/2), '-', color=(0,1,0,1))\n",
    "\n",
    "plt.imshow(norm_map, cmap = 'jet', alpha=0.2, vmin=0, vmax=255)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.6 ('attention_target')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d91fb3ddda745c805933443e6118cc944f530c4e547abcae2a8d6e055d43755f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
